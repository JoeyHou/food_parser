{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '../'\n",
    "\n",
    "data_dir = file_dir + 'data/'\n",
    "raw_data_dir = data_dir + 'raw/'\n",
    "result_dir = data_dir + 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run ../src/food_parser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords, words\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# Updated by Katherine August 23rd 2020\n",
    "stop_words.remove('out') # since pre work out is a valid beverage name\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "\n",
    "wordlist = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting / Making test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>original_logtime_notz</th>\n",
       "      <th>day</th>\n",
       "      <th>local_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>282</td>\n",
       "      <td>alqt15090005</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>09-09-2015 8:20PM JST</td>\n",
       "      <td>2015-09-09 20:20:00</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>20:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>283</td>\n",
       "      <td>alqt15090005</td>\n",
       "      <td>150</td>\n",
       "      <td>Soup</td>\n",
       "      <td>f</td>\n",
       "      <td>09-09-2015 10:38PM JST</td>\n",
       "      <td>2015-09-09 22:38:00</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>22.633333</td>\n",
       "      <td>22:38:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID   unique_code  research_info_id desc_text food_type  \\\n",
       "281  282  alqt15090005               150     Water         w   \n",
       "282  283  alqt15090005               150      Soup         f   \n",
       "\n",
       "           original_logtime original_logtime_notz         day  local_time  \\\n",
       "281   09-09-2015 8:20PM JST   2015-09-09 20:20:00  2015-09-09   20.333333   \n",
       "282  09-09-2015 10:38PM JST   2015-09-09 22:38:00  2015-09-09   22.633333   \n",
       "\n",
       "         time  week_from_start  year  \n",
       "281  20:20:00                1  2015  \n",
       "282  22:38:00                1  2015  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_all_pickle_file = open(raw_data_dir + 'public_0924_basline_usable_expanded.pickle', 'rb')\n",
    "public_all = pickle.load(public_all_pickle_file)\n",
    "public_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = public_all.sample(2000)[['desc_text', 'food_type']].copy().reset_index(drop = True)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.to_csv(raw_data_dir + 'test_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_all.reset_index(drop = True).to_csv(raw_data_dir + 'all_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building improved food annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FoodParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load known grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram_key</th>\n",
       "      <th>food_type</th>\n",
       "      <th>gram_type</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>tag3</th>\n",
       "      <th>tag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chilli</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chile</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gram_key food_type  gram_type tag1 tag2 tag3 tag4\n",
       "0   chilli         f          1  NaN  NaN  NaN  NaN\n",
       "1    chile         f          1  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_grams = pd.read_csv('../resources/combined_gram_set.csv')\n",
    "known_grams.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram_key</th>\n",
       "      <th>food_type</th>\n",
       "      <th>gram_type</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>tag3</th>\n",
       "      <th>tag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>pre workout</td>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "      <td>preworkout</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>preworkout</td>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gram_key food_type  gram_type        tag1 tag2 tag3 tag4\n",
       "981  pre workout         b          2  preworkout  NaN  NaN  NaN\n",
       "982   preworkout         b          1         NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_grams.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_grams_lst = list(known_grams.gram_key.values)\n",
    "known_grams_set = set(known_grams_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['oc', 'amp', 'homemade', 'food', 'foodporn', 'v', 'x', 'recipe', 'best', 'il', 'ever',\n",
    "                'time', 'attempt', 'first', 'ate', 'made', 'home', 'today', 'make', 'friend', 'local',\n",
    "                'new', 'day', 'birthday', 'like', 'amazing', 'de', 'happy', 'year', 'plate', 'video', \n",
    "                'cooked', 'dish', 'house', 'os', 'tried', 'super', 'perfect', 'way', 'delicious', \n",
    "                'night', 'last', 'eating', 'know', \n",
    "                 'mg', 'oz', 'and', 'w', 'x', 'n', 'ml'\n",
    "                 'handful']\n",
    "\n",
    "# Function for removing numbers\n",
    "def handle_numbers(text):\n",
    "    clean_text = text\n",
    "    clean_text = re.sub('[0-9]+\\.[0-9]+', '' , clean_text)\n",
    "    clean_text = re.sub('[0-9]+', '' , clean_text)\n",
    "    clean_text = re.sub('\\s\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "# Function for removing punctuation\n",
    "def drop_punc(text):\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    return clean_text\n",
    "\n",
    "# Remove normal stopwords\n",
    "def remove_stop(my_text):\n",
    "    text_list = my_text.split()\n",
    "    return ' '.join([word for word in text_list if word not in stop_words])\n",
    "\n",
    "def remove_my_stop(text):\n",
    "    text_list = text.split()\n",
    "    return ' '.join([word for word in text_list if word not in my_stop_words])\n",
    "\n",
    "def remove_plural(w):\n",
    "    return wnl.lemmatize(w)\n",
    "\n",
    "def pre_processing(text):\n",
    "    text = text.lower()\n",
    "    text = remove_my_stop(remove_stop(handle_numbers(drop_punc(text)))).strip()\n",
    "    return ' '.join([remove_plural(w) for w in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = public_all.desc_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653926/1653926 [00:50<00:00, 33062.26it/s]\n"
     ]
    }
   ],
   "source": [
    "all_text_cleaned = [pre_processing(tx) for tx in tqdm(all_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653926"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water', 'soup', 'yogurt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_cleaned[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'f', 'b']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_food_type = list(public_all.food_type.values)\n",
    "all_food_type[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 'w'), ('soup', 'f'), ('yogurt', 'b')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_texts = list(zip(all_text_cleaned, all_food_type))\n",
    "training_texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement food mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 'w'), ('soup', 'f'), ('yogurt', 'b')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wholemeal', 'beef taco', 'chicken sausage']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(known_grams_set)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_single_phrase(ph, curr_phrase_dict, known_phrase_dict, known_sub_phrases_set):\n",
    "    # Check known phrases list\n",
    "    try:\n",
    "        known_phrase_dict[ph] += 1\n",
    "        return 0\n",
    "    except:\n",
    "        pass\n",
    "    # Add to current phrase list if its not a subphrase\n",
    "    if ph not in known_sub_phrases_set:\n",
    "        curr_phrase_dict[ph] += 1\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prepare_phrase_base(known_grams_set):\n",
    "#     known_grams_lst = list(known_grams_set)\n",
    "#     known_sub_phrases_lst = []\n",
    "#     tokenlized_known_phrases = [nltk.word_tokenize(ph) for ph in known_grams_lst]\n",
    "#     for ph_length in [1, 2, 3, 4, 5]:\n",
    "#         for tokens in tokenlized_known_phrases:\n",
    "#             if len(tokens) < ph_length:\n",
    "#                 continue\n",
    "#             for tu in nltk.ngrams(tokens, ph_length):\n",
    "#                 known_sub_phrases_lst.append(' '.join(tu))\n",
    "#     known_sub_phrases_set = set(known_sub_phrases_lst)\n",
    "\n",
    "    all_known_phrases_lst = []\n",
    "    for file in ['quad_grams.csv', 'bi_grams.csv', 'uni_grams.csv', 'tri_grams.csv', 'penta_grams.csv']:\n",
    "        curr_df = pd.read_csv('../resources/' + file)\n",
    "        curr_phrases = curr_df.key.values\n",
    "        all_known_phrases_lst += list(curr_phrases)\n",
    "    all_known_sub_phrases_set = set(all_known_phrases_lst)\n",
    "    return all_known_sub_phrases_set\n",
    "\n",
    "def run_training(known_grams_set, training_texts):\n",
    "    '''\n",
    "    Input format:\n",
    "        - known_grams_set: set object, e.g. {'water', 'preworkout', ...}\n",
    "        - training_texts: list object, e.g. [('water', 'w'), ('preworkout', 'b'), ...]\n",
    "    '''\n",
    "    \n",
    "    # Make a spellChecker instance\n",
    "    # spell = SpellChecker()\n",
    "\n",
    "    # Known phrases \n",
    "    known_phrase_dict = dict(zip(known_grams_set, [0] * len(list(known_grams_set))))\n",
    "    known_sub_phrases_set = prepare_phrase_base(known_grams_set)\n",
    "    \n",
    "    # Scanning through the documents\n",
    "    curr_phrase_dict = defaultdict(lambda : 0)\n",
    "\n",
    "    for doc_pair in tqdm(training_texts):\n",
    "        doc, food_type = doc_pair\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        \n",
    "        # Uni-grams\n",
    "        [handle_single_phrase(ph, curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in tokens]\n",
    "        \n",
    "        # N-grams\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 2)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 3)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 4)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 5)]\n",
    "        \n",
    "    return dict(known_phrase_dict), dict(curr_phrase_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653926/1653926 [02:38<00:00, 10438.05it/s]\n"
     ]
    }
   ],
   "source": [
    "known_phrase_dict, curr_phrase_dict = run_training(known_grams_set, training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_phrase_dict = dict(curr_phrase_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items = sorted(curr_phrase_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_items = [pair for pair in sorted_items if pair[1] > 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1731"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', 12170),\n",
       " ('herbal', 8492),\n",
       " ('b', 7861),\n",
       " ('herbal tea', 7603),\n",
       " ('g', 7277),\n",
       " ('free', 7261),\n",
       " ('tea milk', 6322),\n",
       " ('no', 5634),\n",
       " ('med', 4819),\n",
       " ('cider', 4772),\n",
       " ('stevia', 4157),\n",
       " ('glass water', 4075),\n",
       " ('vitamin c', 4008),\n",
       " ('bannana', 3887),\n",
       " ('unsweetened', 3612),\n",
       " ('bulletproof', 3416),\n",
       " ('apple cider', 3360),\n",
       " ('crisp', 3353),\n",
       " ('kefir', 2986),\n",
       " ('bulletproof coffee', 2901),\n",
       " ('tsp', 2878),\n",
       " ('coffe', 2867),\n",
       " ('cider vinegar', 2866),\n",
       " ('keto', 2866),\n",
       " ('broth', 2839),\n",
       " ('no sugar', 2812),\n",
       " ('heavy', 2792),\n",
       " ('mcg', 2771),\n",
       " ('soy milk', 2698),\n",
       " ('stick', 2686),\n",
       " ('ml water', 2686),\n",
       " ('pumpkin seed', 2601),\n",
       " ('complex', 2581),\n",
       " ('chickpea', 2520),\n",
       " ('mct oil', 2461),\n",
       " ('oat milk', 2387),\n",
       " ('gluten free', 2330),\n",
       " ('sugar free', 2330),\n",
       " ('k', 2328),\n",
       " ('pill', 2282)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_items[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_known_phrases_lst = []\n",
    "# for file in ['quad_grams.csv', 'bi_grams.csv', 'uni_grams.csv', 'tri_grams.csv', 'penta_grams.csv']:\n",
    "#     curr_df = pd.read_csv('../resources/' + file)\n",
    "#     curr_phrases = curr_df.key.values\n",
    "#     all_known_phrases_lst += list(curr_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_known_phrases_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = [pair[0] for pair in sorted_items]\n",
    "gram_type = [len(nltk.word_tokenize(pair[0])) for pair in sorted_items]\n",
    "count = [pair[1] for pair in sorted_items]\n",
    "\n",
    "additional_phrases = pd.DataFrame({\n",
    "    'key': key,\n",
    "    'gram_type': gram_type,\n",
    "    'count': count\n",
    "}).sort_values(by = 'gram_type', ascending = False)\n",
    "\n",
    "additional_phrases.to_csv('../data/result/additional_phrases.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ml', 'herbal', 'b'], [1, 1, 1], [12170, 8492, 7861])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key[:3], gram_type[:3], count[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ml', 'herbal', 'b'], 1731)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_phrase_bank = [pair[0] for pair in sorted_items]\n",
    "curr_phrase_bank[:3], len(curr_phrase_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1731 [00:00<01:04, 26.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Length of curr_phrase_bank: 1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1731/1731 [03:24<00:00,  8.47it/s]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-52-2270ebb5ba5d>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-2270ebb5ba5d>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "print('=> Length of curr_phrase_bank:', len(curr_phrase_bank))\n",
    "    \n",
    "tmp_mask = np.array([True] * len(curr_phrase_bank))\n",
    "curr_phrase_bank = np.array(curr_phrase_bank)\n",
    "    \n",
    "curr_idx = 0\n",
    "for ph in tqdm(curr_phrase_bank):\n",
    "    tokens = nltk.word_tokenize(ph)\n",
    "    curr_gram_length = len(tokens)\n",
    "    for other_ph in curr_phrase_bank:\n",
    "        if ph == other_ph:\n",
    "            continue\n",
    "            \n",
    "        other_ph_tokens = nltk.word_tokenize(other_ph)\n",
    "        if len(other_ph_tokens) < curr_gram_length:\n",
    "            continue\n",
    "            \n",
    "        n_grams = [' '.join(gram) for gram in nltk.ngrams(other_ph_tokens, curr_gram_length)]\n",
    "        if ph in n_grams:\n",
    "            tmp_mask[curr_idx] = False\n",
    "            break\n",
    "    curr_idx += 1\n",
    "if sum(tmp_mask) == len(tmp_mask):\n",
    "    print('    => No Change to current phrase bank!')\n",
    "#     break\n",
    "curr_phrase_bank = curr_phrase_bank[tmp_mask]\n",
    "print(len(curr_phrase_bank))\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413\n"
     ]
    }
   ],
   "source": [
    "curr_phrase_bank = curr_phrase_bank[tmp_mask]\n",
    "\n",
    "print(len(curr_phrase_bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.810878"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.189122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
