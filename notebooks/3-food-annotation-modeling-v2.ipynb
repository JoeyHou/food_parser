{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict \n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '../'\n",
    "\n",
    "data_dir = file_dir + 'test_data/'\n",
    "raw_data_dir = data_dir + 'raw/'\n",
    "result_dir = data_dir + 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(1, '../foodparser/')\n",
    "\n",
    "from foodparser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FoodParser()\n",
    "fp.initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fp.run_setup()\n",
    "'scramble' in fp.food_type_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords, words\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# Updated by Katherine August 23rd 2020\n",
    "stop_words.remove('out') # since pre work out is a valid beverage name\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "\n",
    "wordlist = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting / Making test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>original_logtime_notz</th>\n",
       "      <th>day</th>\n",
       "      <th>local_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>282</td>\n",
       "      <td>alqt15090005</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>09-09-2015 8:20PM JST</td>\n",
       "      <td>2015-09-09 20:20:00</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>20:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>283</td>\n",
       "      <td>alqt15090005</td>\n",
       "      <td>150</td>\n",
       "      <td>Soup</td>\n",
       "      <td>f</td>\n",
       "      <td>09-09-2015 10:38PM JST</td>\n",
       "      <td>2015-09-09 22:38:00</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>22.633333</td>\n",
       "      <td>22:38:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID   unique_code  research_info_id desc_text food_type  \\\n",
       "281  282  alqt15090005               150     Water         w   \n",
       "282  283  alqt15090005               150      Soup         f   \n",
       "\n",
       "           original_logtime original_logtime_notz         day  local_time  \\\n",
       "281   09-09-2015 8:20PM JST   2015-09-09 20:20:00  2015-09-09   20.333333   \n",
       "282  09-09-2015 10:38PM JST   2015-09-09 22:38:00  2015-09-09   22.633333   \n",
       "\n",
       "         time  week_from_start  year  \n",
       "281  20:20:00                1  2015  \n",
       "282  22:38:00                1  2015  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_all_pickle_file = open(raw_data_dir + 'public_0924_basline_usable_expanded.pickle', 'rb')\n",
    "public_all = pickle.load(public_all_pickle_file)\n",
    "public_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13921"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_all.unique_code.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153330, 12)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_all.query('food_type in [\"b\", \"f\"]').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = public_all.sample(2000)[['desc_text', 'food_type']].copy().reset_index(drop = True)\n",
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.to_csv(raw_data_dir + 'test_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_all.reset_index(drop = True).to_csv(raw_data_dir + 'all_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1653926, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_all = public_all.query('food_type == \"m\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loggings = public_all.desc_text.values\n",
    "all_entries = []\n",
    "for l in all_loggings:\n",
    "    if ',' in l:\n",
    "        for entry in l.split(','):\n",
    "            all_entries.append(entry.strip())\n",
    "    else:\n",
    "        all_entries.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205623"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entries_length = ([len(i.split()) for i in all_entries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2065"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(all_entries_length) > 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional training texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1650, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unknown_phrases_df = pd.read_csv(result_dir + 'all_unknown_phrases.csv').dropna()\n",
    "all_unknown_phrases_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unknown_phrases</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breakfast</td>\n",
       "      <td>6096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dinner</td>\n",
       "      <td>5784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unknown_phrases  counts\n",
       "0       breakfast    6096\n",
       "1          dinner    5784"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unknown_phrases_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building improved food annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FoodParser()\n",
    "fp.initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load known grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram_key</th>\n",
       "      <th>food_type</th>\n",
       "      <th>gram_type</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>tag3</th>\n",
       "      <th>tag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chilli</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chile</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gram_key food_type  gram_type tag1 tag2 tag3 tag4\n",
       "0   chilli         f          1  NaN  NaN  NaN  NaN\n",
       "1    chile         f          1  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_grams = pd.read_csv('../foodparser/data/combined_gram_set.csv')\n",
    "known_grams.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram_key</th>\n",
       "      <th>food_type</th>\n",
       "      <th>gram_type</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>tag3</th>\n",
       "      <th>tag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>fat</td>\n",
       "      <td>n</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>mashed</td>\n",
       "      <td>modifier</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gram_key food_type  gram_type tag1 tag2 tag3 tag4\n",
       "2380      fat         n          1  NaN  NaN  NaN  NaN\n",
       "2381   mashed  modifier          1  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_grams.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_grams_lst = list(known_grams.gram_key.values)\n",
    "known_grams_set = set(known_grams_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['oc', 'amp', 'homemade', 'food', 'foodporn', 'v', 'x', 'recipe', 'best', 'il', 'ever',\n",
    "                'time', 'attempt', 'first', 'ate', 'made', 'home', 'today', 'make', 'friend', 'local',\n",
    "                'new', 'day', 'birthday', 'like', 'amazing', 'de', 'happy', 'year', 'plate', 'video', \n",
    "                'cooked', 'dish', 'house', 'os', 'tried', 'super', 'perfect', 'way', 'delicious', \n",
    "                'night', 'last', 'eating', 'know', \n",
    "                 'mg', 'oz', 'and', 'w', 'x', 'n', 'ml'\n",
    "                 'handful']\n",
    "\n",
    "# Function for removing numbers\n",
    "def handle_numbers(text):\n",
    "    clean_text = text\n",
    "    clean_text = re.sub('[0-9]+\\.[0-9]+', '' , clean_text)\n",
    "    clean_text = re.sub('[0-9]+', '' , clean_text)\n",
    "    clean_text = re.sub('\\s\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "# Function for removing punctuation\n",
    "def drop_punc(text):\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    return clean_text\n",
    "\n",
    "# Remove normal stopwords\n",
    "def remove_stop(my_text):\n",
    "    text_list = my_text.split()\n",
    "    return ' '.join([word for word in text_list if word not in stop_words])\n",
    "\n",
    "def remove_my_stop(text):\n",
    "    text_list = text.split()\n",
    "    return ' '.join([word for word in text_list if word not in my_stop_words])\n",
    "\n",
    "def remove_plural(w):\n",
    "    return wnl.lemmatize(w)\n",
    "\n",
    "def pre_processing(text):\n",
    "    text = text.lower()\n",
    "    text = remove_my_stop(remove_stop(handle_numbers(drop_punc(text)))).strip()\n",
    "    return ' '.join([remove_plural(w) for w in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = public_all.desc_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105914/105914 [00:04<00:00, 23224.12it/s]\n"
     ]
    }
   ],
   "source": [
    "all_text_cleaned = [pre_processing(tx) for tx in tqdm(all_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105914"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chiraita extract', 'chiraita extract', 'chiraita extract']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_cleaned[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m', 'm', 'm']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_food_type = list(public_all.food_type.values)\n",
    "all_food_type[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chiraita extract', 'm'),\n",
       " ('chiraita extract', 'm'),\n",
       " ('chiraita extract', 'm')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_texts = list(zip(all_text_cleaned, all_food_type))\n",
    "training_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement food mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thorne protein powder scoop', 'halibut', 'tom']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(known_grams_set)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_single_phrase(ph, curr_phrase_dict, known_phrase_dict, known_sub_phrases_set):\n",
    "    # Check known phrases list\n",
    "    try:\n",
    "        known_phrase_dict[ph] += 1\n",
    "        return 0\n",
    "    except:\n",
    "        pass\n",
    "    # Add to current phrase list if its not a subphrase\n",
    "    if ph not in known_sub_phrases_set:\n",
    "        curr_phrase_dict[ph] += 1\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prepare_phrase_base(known_grams_set):\n",
    "    all_known_phrases_lst = []\n",
    "    for file in ['quad_grams.csv', 'bi_grams.csv', 'uni_grams.csv', 'tri_grams.csv', 'penta_grams.csv']:\n",
    "        curr_df = pd.read_csv('../foodparser/data/annotations/' + file)\n",
    "        curr_phrases = curr_df.key.values\n",
    "        all_known_phrases_lst += list(curr_phrases)\n",
    "    all_known_sub_phrases_set = set(all_known_phrases_lst)\n",
    "    return all_known_sub_phrases_set\n",
    "\n",
    "def run_training(known_grams_set, training_texts):\n",
    "    '''\n",
    "    Input format:\n",
    "        - known_grams_set: set object, e.g. {'water', 'preworkout', ...}\n",
    "        - training_texts: list object, e.g. [('water', 'w'), ('preworkout', 'b'), ...]\n",
    "    '''\n",
    "    \n",
    "    # Make a spellChecker instance\n",
    "    # spell = SpellChecker()\n",
    "\n",
    "    # Known phrases \n",
    "    known_phrase_dict = dict(zip(known_grams_set, [0] * len(list(known_grams_set))))\n",
    "    known_sub_phrases_set = prepare_phrase_base(known_grams_set)\n",
    "    \n",
    "    # Scanning through the documents\n",
    "    curr_phrase_dict = defaultdict(lambda : 0)\n",
    "\n",
    "    for doc_pair in tqdm(training_texts):\n",
    "        if len(doc_pair) == 2:\n",
    "            doc, food_type = doc_pair\n",
    "        else:\n",
    "            doc = doc_pair\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        \n",
    "        # Uni-grams\n",
    "        [handle_single_phrase(ph, curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in tokens]\n",
    "        \n",
    "        # N-grams\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 2)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 3)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 4)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 5)]\n",
    "        \n",
    "    return dict(known_phrase_dict), dict(curr_phrase_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105914/105914 [00:11<00:00, 9021.79it/s]\n",
      "100%|██████████| 1650/1650 [00:00<00:00, 12427.99it/s]\n"
     ]
    }
   ],
   "source": [
    "known_phrase_dict, curr_phrase_dict = run_training(known_grams_set, training_texts)\n",
    "add_known_phrase_dict, add_curr_phrase_dict = run_training(known_grams_set, all_unknown_phrases_df.unknown_phrases.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2382, 159990)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_phrase_dict = dict(curr_phrase_dict)\n",
    "known_phrase_dict = dict(known_phrase_dict)\n",
    "\n",
    "sorted_old_phrases = sorted(known_phrase_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_new_phrases = sorted(curr_phrase_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "len(sorted_old_phrases), len(sorted_new_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_curr_phrase_dict = dict(add_curr_phrase_dict)\n",
    "add_known_phrase_dict = dict(add_known_phrase_dict)\n",
    "\n",
    "sorted_old_phrases_add = sorted(add_known_phrase_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_new_phrases_add = sorted(add_curr_phrase_dict.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 3),\n",
       " ('time', 3),\n",
       " ('night', 2),\n",
       " ('kiss', 2),\n",
       " ('pico', 2),\n",
       " ('mein', 2),\n",
       " ('picolinate', 2),\n",
       " ('sauvignon', 2),\n",
       " ('folate', 2),\n",
       " ('flavor', 2),\n",
       " ('ox', 2),\n",
       " ('carnitine', 2),\n",
       " ('l carnitine', 2),\n",
       " ('luna', 2),\n",
       " ('spark', 2),\n",
       " ('lithium', 2),\n",
       " ('choline', 2),\n",
       " ('lady', 2),\n",
       " ('chi', 2),\n",
       " ('ketone', 2),\n",
       " ('epic', 2),\n",
       " ('kim', 2),\n",
       " ('jimmy', 2),\n",
       " ('tartar', 2),\n",
       " ('cool', 2),\n",
       " ('infused', 2),\n",
       " ('choi', 2),\n",
       " ('gratin', 2),\n",
       " ('beta', 2),\n",
       " ('activated', 2),\n",
       " ('rhonda', 2),\n",
       " ('patrick', 2),\n",
       " ('rhonda patrick', 2),\n",
       " ('angel', 2),\n",
       " ('cross', 1),\n",
       " ('triscuits', 1),\n",
       " ('stock', 1),\n",
       " ('iceberg', 1),\n",
       " ('shallot', 1),\n",
       " ('caper', 1),\n",
       " ('bush', 1),\n",
       " ('cous cous', 1),\n",
       " ('caffeinated', 1),\n",
       " ('scallion', 1),\n",
       " ('pringles', 1),\n",
       " ('hashbrowns', 1),\n",
       " ('crestor', 1),\n",
       " ('sharp', 1),\n",
       " ('goldfish', 1),\n",
       " ('astaxanthin', 1),\n",
       " ('loratadine', 1),\n",
       " ('brioche', 1),\n",
       " ('chive', 1),\n",
       " ('sangria', 1),\n",
       " ('cymbalta', 1),\n",
       " ('sashimi', 1),\n",
       " ('perrier', 1),\n",
       " ('eau', 1),\n",
       " ('prednisone', 1),\n",
       " ('allegra', 1),\n",
       " ('bud', 1),\n",
       " ('museli', 1),\n",
       " ('lacroix', 1),\n",
       " ('parfait', 1),\n",
       " ('grated', 1),\n",
       " ('tiramisu', 1),\n",
       " ('prozac', 1),\n",
       " ('carvedilol', 1),\n",
       " ('powerade', 1),\n",
       " ('cappucino', 1),\n",
       " ('joghurt', 1),\n",
       " ('shepherd', 1),\n",
       " ('echinacea', 1),\n",
       " ('anchovy', 1),\n",
       " ('mayonaise', 1),\n",
       " ('stroganoff', 1),\n",
       " ('peptide', 1),\n",
       " ('sundried', 1),\n",
       " ('medjool', 1),\n",
       " ('mung', 1),\n",
       " ('propranolol', 1),\n",
       " ('valsartan', 1),\n",
       " ('advair', 1),\n",
       " ('chobani', 1),\n",
       " ('seasoning', 1),\n",
       " ('co q', 1),\n",
       " ('wort', 1),\n",
       " ('st john', 1),\n",
       " ('john wort', 1),\n",
       " ('st john wort', 1),\n",
       " ('currant', 1),\n",
       " ('wendys', 1),\n",
       " ('pod', 1),\n",
       " ('buttermilk', 1),\n",
       " ('meloxicam', 1),\n",
       " ('sheep', 1),\n",
       " ('tapioca', 1),\n",
       " ('mil', 1),\n",
       " ('chestnut', 1),\n",
       " ('sour dough', 1),\n",
       " ('sticky', 1),\n",
       " ('pound', 1),\n",
       " ('ratatouille', 1),\n",
       " ('soylent', 1),\n",
       " ('rhodiola', 1),\n",
       " ('watercress', 1),\n",
       " ('beyond', 1),\n",
       " ('beverage', 1),\n",
       " ('breakfast bar', 1),\n",
       " ('premier', 1),\n",
       " ('snow', 1),\n",
       " ('daal', 1),\n",
       " ('stewed', 1),\n",
       " ('empanada', 1),\n",
       " ('con carne', 1),\n",
       " ('tblsp', 1),\n",
       " ('schnitzel', 1),\n",
       " ('tbl', 1),\n",
       " ('honeydew', 1),\n",
       " ('zantac', 1),\n",
       " ('sorbet', 1),\n",
       " ('caffe', 1),\n",
       " ('pressed', 1),\n",
       " ('arepa', 1),\n",
       " ('poppy', 1),\n",
       " ('wasabi', 1),\n",
       " ('bubble', 1),\n",
       " ('focaccia', 1),\n",
       " ('rainbow', 1),\n",
       " ('babybel', 1),\n",
       " ('magnum', 1),\n",
       " ('caf', 1),\n",
       " ('spironolactone', 1),\n",
       " ('havarti', 1),\n",
       " ('macaroon', 1),\n",
       " ('squid', 1),\n",
       " ('craisins', 1),\n",
       " ('ciabatta', 1),\n",
       " ('snack bar', 1),\n",
       " ('kalamata', 1),\n",
       " ('mimosa', 1),\n",
       " ('prescription', 1),\n",
       " ('loaded', 1),\n",
       " ('mochi', 1),\n",
       " ('dove', 1),\n",
       " ('preserve', 1),\n",
       " ('chunk', 1),\n",
       " ('curried', 1),\n",
       " ('straw', 1),\n",
       " ('whipping', 1),\n",
       " ('lolly', 1),\n",
       " ('molasses', 1),\n",
       " ('refill', 1),\n",
       " ('chew', 1),\n",
       " ('qt', 1),\n",
       " ('glas', 1),\n",
       " ('bloody', 1),\n",
       " ('bloody mary', 1),\n",
       " ('vyvanse', 1),\n",
       " ('twix', 1),\n",
       " ('clove', 1),\n",
       " ('sundae', 1),\n",
       " ('brat', 1),\n",
       " ('prolon', 1),\n",
       " ('alfalfa', 1),\n",
       " ('plant based', 1),\n",
       " ('musli', 1),\n",
       " ('dahl', 1),\n",
       " ('activia', 1),\n",
       " ('decafe', 1),\n",
       " ('kat', 1),\n",
       " ('kit kat', 1),\n",
       " ('granny', 1),\n",
       " ('smith', 1),\n",
       " ('granny smith', 1),\n",
       " ('fourth', 1),\n",
       " ('colby jack', 1),\n",
       " ('bailey', 1),\n",
       " ('hardboiled', 1),\n",
       " ('weetbix', 1),\n",
       " ('nd', 1),\n",
       " ('tinned', 1),\n",
       " ('spiced', 1),\n",
       " ('vietnamese', 1),\n",
       " ('eliquis', 1),\n",
       " ('fashioned', 1),\n",
       " ('amoxicillin', 1),\n",
       " ('marmelade', 1),\n",
       " ('hershey kiss', 1),\n",
       " ('tzatziki', 1),\n",
       " ('broccolini', 1),\n",
       " ('vera', 1),\n",
       " ('bubly', 1),\n",
       " ('berocca', 1),\n",
       " ('spearmint', 1),\n",
       " ('bruschetta', 1),\n",
       " ('corona', 1),\n",
       " ('fraiche', 1),\n",
       " ('mucinex', 1),\n",
       " ('easy', 1),\n",
       " ('ozs', 1),\n",
       " ('tempura', 1),\n",
       " ('pattie', 1),\n",
       " ('rusk', 1),\n",
       " ('kerrygold', 1),\n",
       " ('portobello', 1),\n",
       " ('chef', 1),\n",
       " ('allopurinol', 1),\n",
       " ('merlot', 1),\n",
       " ('milky', 1),\n",
       " ('pepitas', 1),\n",
       " ('haddock', 1),\n",
       " ('baclofen', 1),\n",
       " ('biryani', 1),\n",
       " ('lamictal', 1),\n",
       " ('cobbler', 1),\n",
       " ('amy', 1),\n",
       " (\"amy 's\", 1),\n",
       " ('shakeology', 1),\n",
       " ('korma', 1),\n",
       " ('cutlet', 1),\n",
       " ('ranitidine', 1),\n",
       " ('evoo', 1),\n",
       " ('cigarette', 1),\n",
       " ('tramadol', 1),\n",
       " ('hand', 1),\n",
       " ('medley', 1),\n",
       " ('asprin', 1),\n",
       " ('cluster', 1),\n",
       " ('gallo', 1),\n",
       " ('pico gallo', 1),\n",
       " ('hydrolyzed', 1),\n",
       " ('ginkgo biloba', 1),\n",
       " ('miller', 1),\n",
       " ('miller lite', 1),\n",
       " ('halo', 1),\n",
       " ('carbonara', 1),\n",
       " ('jicama', 1),\n",
       " ('vit e', 1),\n",
       " ('toastie', 1),\n",
       " ('chick fil', 1),\n",
       " ('redbush', 1),\n",
       " ('acidophilus', 1),\n",
       " ('paella', 1),\n",
       " ('fanta', 1),\n",
       " ('gelatin', 1),\n",
       " ('modafinil', 1),\n",
       " ('kielbasa', 1),\n",
       " ('sparking', 1),\n",
       " ('celexa', 1),\n",
       " ('prilosec', 1),\n",
       " ('zuchinni', 1),\n",
       " ('isolate', 1),\n",
       " ('orzo', 1),\n",
       " ('methylphenidate', 1),\n",
       " ('gala', 1),\n",
       " ('octopus', 1),\n",
       " ('trazadone', 1),\n",
       " ('bottled', 1),\n",
       " ('farro', 1),\n",
       " ('thyme', 1),\n",
       " ('chic', 1),\n",
       " ('squeezed', 1),\n",
       " ('nexium', 1),\n",
       " ('trader', 1),\n",
       " ('trader joes', 1),\n",
       " ('sallad', 1),\n",
       " ('filled', 1),\n",
       " ('effexor', 1),\n",
       " ('supreme', 1),\n",
       " ('back', 1),\n",
       " ('camembert', 1),\n",
       " ('kitkat', 1),\n",
       " ('kfc', 1),\n",
       " ('elderberry', 1),\n",
       " ('bred', 1),\n",
       " ('baklava', 1),\n",
       " ('satay', 1),\n",
       " ('weak', 1),\n",
       " ('broad', 1),\n",
       " ('cc', 1),\n",
       " ('chow mein', 1),\n",
       " ('wrapped', 1),\n",
       " ('soaked', 1),\n",
       " ('kcal', 1),\n",
       " ('tossed', 1),\n",
       " ('grill', 1),\n",
       " ('skewer', 1),\n",
       " ('animal', 1),\n",
       " ('grassfed', 1),\n",
       " ('lox', 1),\n",
       " ('monk', 1),\n",
       " ('caff', 1),\n",
       " ('rich', 1),\n",
       " ('liquorice', 1),\n",
       " ('yorkshire', 1),\n",
       " ('lg', 1),\n",
       " ('southwest', 1),\n",
       " ('bass', 1),\n",
       " ('sea bass', 1),\n",
       " ('fenugreek', 1),\n",
       " ('duloxetine', 1),\n",
       " ('gherkin', 1),\n",
       " ('tulsi', 1),\n",
       " ('punch', 1),\n",
       " ('nicotine', 1),\n",
       " ('milled', 1),\n",
       " ('hashbrown', 1),\n",
       " ('fresca', 1),\n",
       " ('guinness', 1),\n",
       " ('asa', 1),\n",
       " ('creamed', 1),\n",
       " ('saute', 1),\n",
       " ('multiple', 1),\n",
       " ('oxycodone', 1),\n",
       " ('fl', 1),\n",
       " ('goulash', 1),\n",
       " ('pocket', 1),\n",
       " ('shawarma', 1),\n",
       " ('salty', 1),\n",
       " ('malt', 1),\n",
       " ('diced', 1),\n",
       " ('inulin', 1),\n",
       " ('glazed', 1),\n",
       " ('compote', 1),\n",
       " ('bitter', 1),\n",
       " ('pravastatin', 1),\n",
       " ('salade', 1),\n",
       " ('excedrin', 1),\n",
       " ('sage', 1),\n",
       " ('gazpacho', 1),\n",
       " ('kabob', 1),\n",
       " ('alkaline', 1),\n",
       " ('jackfruit', 1),\n",
       " ('te', 1),\n",
       " ('germ', 1),\n",
       " ('venlafaxine', 1),\n",
       " ('singulair', 1),\n",
       " ('hair skin', 1),\n",
       " ('skin nail', 1),\n",
       " ('hair skin nail', 1),\n",
       " ('cytomel', 1),\n",
       " ('key', 1),\n",
       " ('acorn', 1),\n",
       " ('ala', 1),\n",
       " ('nori', 1),\n",
       " ('ww', 1),\n",
       " ('vitamine', 1),\n",
       " ('hydrolysate', 1),\n",
       " ('bragg', 1),\n",
       " ('xarelto', 1),\n",
       " ('clonazepam', 1),\n",
       " ('real', 1),\n",
       " ('four', 1),\n",
       " ('herbalife', 1),\n",
       " ('naked', 1),\n",
       " ('haloumi', 1),\n",
       " ('airborne', 1),\n",
       " ('hrt', 1),\n",
       " ('restore', 1),\n",
       " ('almondmilk', 1),\n",
       " ('kraut', 1),\n",
       " ('moscow', 1),\n",
       " ('mule', 1),\n",
       " ('moscow mule', 1),\n",
       " ('indomethacin', 1),\n",
       " ('spear', 1),\n",
       " ('propel', 1),\n",
       " ('eyed', 1),\n",
       " ('marinated', 1),\n",
       " ('chromium picolinate', 1),\n",
       " ('udon', 1),\n",
       " ('lipton', 1),\n",
       " ('pinch', 1),\n",
       " ('bisoprolol', 1),\n",
       " ('san', 1),\n",
       " ('ceylon', 1),\n",
       " ('dhal', 1),\n",
       " ('port', 1),\n",
       " ('gaba', 1),\n",
       " ('atkins bar', 1),\n",
       " ('gingerbread', 1),\n",
       " ('tamsulosin', 1),\n",
       " ('shiitake', 1),\n",
       " ('amaranth', 1),\n",
       " ('muscle', 1),\n",
       " ('epa dha', 1),\n",
       " ('ryvita', 1),\n",
       " ('tamari', 1),\n",
       " ('rise', 1),\n",
       " ('vsl', 1),\n",
       " ('powdered', 1),\n",
       " ('dragon', 1),\n",
       " ('natto', 1),\n",
       " ('cuban', 1),\n",
       " ('braised', 1),\n",
       " ('acetaminophen', 1),\n",
       " ('cardamom', 1),\n",
       " ('meringue', 1),\n",
       " ('kohlrabi', 1),\n",
       " ('np', 1),\n",
       " ('np thyroid', 1),\n",
       " ('soba', 1),\n",
       " ('gyoza', 1),\n",
       " ('cough drop', 1),\n",
       " ('pilaf', 1),\n",
       " ('besylate', 1),\n",
       " ('brandy', 1),\n",
       " ('bulgur', 1),\n",
       " ('canadian', 1),\n",
       " ('hole', 1),\n",
       " ('stalk', 1),\n",
       " ('finasteride', 1),\n",
       " ('parmigiana', 1),\n",
       " ('gorgonzola', 1),\n",
       " ('table', 1),\n",
       " ('boneless', 1),\n",
       " ('yes', 1),\n",
       " ('paratha', 1),\n",
       " ('tostitos', 1),\n",
       " ('bircher', 1),\n",
       " ('areds', 1),\n",
       " ('glutenfree', 1),\n",
       " ('capuccino', 1),\n",
       " ('grigio', 1),\n",
       " ('pregnenolone', 1),\n",
       " ('energy ball', 1),\n",
       " ('style', 1),\n",
       " ('yog', 1),\n",
       " ('fuji', 1),\n",
       " ('testosterone', 1),\n",
       " ('dunkin', 1),\n",
       " ('glipizide', 1),\n",
       " ('micronutrient', 1),\n",
       " ('humous', 1),\n",
       " ('pct', 1),\n",
       " ('legume', 1),\n",
       " ('hempseed', 1),\n",
       " ('fage', 1),\n",
       " ('pumpernickel', 1),\n",
       " ('mush', 1),\n",
       " ('sudafed', 1),\n",
       " ('serve', 1),\n",
       " ('kashi', 1),\n",
       " ('mk', 1),\n",
       " ('micro', 1),\n",
       " ('butte', 1),\n",
       " ('cheez', 1),\n",
       " ('sloppy', 1),\n",
       " ('joe', 1),\n",
       " ('sloppy joe', 1),\n",
       " ('gumbo', 1),\n",
       " ('dirty', 1),\n",
       " ('triamterene', 1),\n",
       " ('brazilian', 1),\n",
       " ('amitriptyline', 1),\n",
       " ('ug', 1),\n",
       " ('topping', 1),\n",
       " ('poppyseed', 1),\n",
       " ('german', 1),\n",
       " ('thyroxin', 1),\n",
       " ('cheezits', 1),\n",
       " ('frappuccino', 1),\n",
       " ('party', 1),\n",
       " ('food', 1),\n",
       " ('replacement', 1),\n",
       " ('aioli', 1),\n",
       " ('bile', 1),\n",
       " ('ox bile', 1),\n",
       " ('chewing', 1),\n",
       " ('tiger', 1),\n",
       " ('crystal lite', 1),\n",
       " ('pollen', 1),\n",
       " ('bee pollen', 1),\n",
       " ('etc', 1),\n",
       " ('mignon', 1),\n",
       " ('onz', 1),\n",
       " ('pomelo', 1),\n",
       " ('bolognaise', 1),\n",
       " ('hydrochloride', 1),\n",
       " ('stout', 1),\n",
       " ('rock', 1),\n",
       " ('lyrica', 1),\n",
       " ('sake', 1),\n",
       " ('doxycycline', 1),\n",
       " ('bedtime', 1),\n",
       " ('flavoured', 1),\n",
       " ('hydrocortisone', 1),\n",
       " ('steam', 1),\n",
       " ('fenofibrate', 1),\n",
       " ('jambalaya', 1),\n",
       " ('alpha brain', 1),\n",
       " ('citrus', 1),\n",
       " ('sponge', 1),\n",
       " ('celebrex', 1),\n",
       " ('nondairy', 1),\n",
       " ('mulberry', 1),\n",
       " ('puerh', 1),\n",
       " ('org', 1),\n",
       " ('lemongrass', 1),\n",
       " ('ui', 1),\n",
       " ('br', 1),\n",
       " ('acyclovir', 1),\n",
       " ('mio', 1),\n",
       " ('bologna', 1),\n",
       " ('brulee', 1),\n",
       " ('leafy', 1),\n",
       " ('stool', 1),\n",
       " ('softener', 1),\n",
       " ('stool softener', 1),\n",
       " ('coenzyme', 1),\n",
       " ('coenzyme q', 1),\n",
       " ('claw', 1),\n",
       " ('veal', 1),\n",
       " ('candied', 1),\n",
       " ('trazodone', 1),\n",
       " ('snapple', 1),\n",
       " ('buspirone', 1),\n",
       " ('point', 1),\n",
       " ('moon', 1),\n",
       " ('junk', 1),\n",
       " ('silk', 1),\n",
       " ('tamoxifen', 1),\n",
       " ('round', 1),\n",
       " ('pain au', 1),\n",
       " ('ambien', 1),\n",
       " ('ccm', 1),\n",
       " ('appetizer', 1),\n",
       " ('borage', 1),\n",
       " ('fuel', 1),\n",
       " ('iron tablet', 1),\n",
       " ('pt', 1),\n",
       " ('calzone', 1),\n",
       " ('celeriac', 1),\n",
       " ('kumara', 1),\n",
       " ('caviar', 1),\n",
       " ('bark', 1),\n",
       " ('flan', 1),\n",
       " ('scampi', 1),\n",
       " ('seitan', 1),\n",
       " ('curcuma', 1),\n",
       " ('bliss', 1),\n",
       " ('bliss ball', 1),\n",
       " ('spam', 1),\n",
       " ('estrogen', 1),\n",
       " ('flank', 1),\n",
       " ('popeyes', 1),\n",
       " ('tin', 1),\n",
       " ('krispies', 1),\n",
       " ('saw', 1),\n",
       " ('palmetto', 1),\n",
       " ('saw palmetto', 1),\n",
       " ('athletic', 1),\n",
       " ('wasa', 1),\n",
       " ('ca', 1),\n",
       " ('malate', 1),\n",
       " ('coffie', 1),\n",
       " ('cutie', 1),\n",
       " ('out', 1),\n",
       " ('losartin', 1),\n",
       " ('sambar', 1),\n",
       " ('singular', 1),\n",
       " ('tum', 1),\n",
       " ('plaquenil', 1),\n",
       " ('brunch', 1),\n",
       " ('lait', 1),\n",
       " ('cafe au', 1),\n",
       " ('au lait', 1),\n",
       " ('cafe au lait', 1),\n",
       " ('irbesartan', 1),\n",
       " ('broc', 1),\n",
       " ('range', 1),\n",
       " ('fibre', 1),\n",
       " ('lima', 1),\n",
       " ('wheatgrass', 1),\n",
       " ('assorted', 1),\n",
       " ('shortcake', 1),\n",
       " ('topiramate', 1),\n",
       " ('meriva', 1),\n",
       " ('nectar', 1),\n",
       " ('methimazole', 1),\n",
       " ('amla', 1),\n",
       " ('bengal', 1),\n",
       " ('zma', 1),\n",
       " ('nicotinamide', 1),\n",
       " ('riboside', 1),\n",
       " ('nicotinamide riboside', 1),\n",
       " ('domino', 1),\n",
       " ('dk', 1),\n",
       " ('milo', 1),\n",
       " ('bisque', 1),\n",
       " ('acetyl l', 1),\n",
       " ('acetyl l carnitine', 1),\n",
       " ('puffed', 1),\n",
       " ('triphala', 1),\n",
       " ('cordyceps', 1),\n",
       " ('tropical', 1),\n",
       " ('fit', 1),\n",
       " ('tapa', 1),\n",
       " ('reuben', 1),\n",
       " ('pimento', 1),\n",
       " ('euthyrox', 1),\n",
       " ('luna bar', 1),\n",
       " ('swedish', 1),\n",
       " ('aged', 1),\n",
       " ('diluted', 1),\n",
       " ('welbutrin', 1),\n",
       " ('microwave', 1),\n",
       " ('mct powder', 1),\n",
       " ('pink himalayan', 1),\n",
       " ('bluberries', 1),\n",
       " ('nyquil', 1),\n",
       " ('sticker', 1),\n",
       " ('pot sticker', 1),\n",
       " ('casein', 1),\n",
       " ('potstickers', 1),\n",
       " ('boil', 1),\n",
       " ('strudel', 1),\n",
       " ('cava', 1),\n",
       " ('afternoon snack', 1),\n",
       " ('pounder', 1),\n",
       " ('hormone', 1),\n",
       " ('monohydrate', 1),\n",
       " ('eggnog', 1),\n",
       " ('horseradish', 1),\n",
       " ('butt', 1),\n",
       " ('tootsie', 1),\n",
       " ('dha epa', 1),\n",
       " ('mega', 1),\n",
       " ('philadelphia', 1),\n",
       " ('vine', 1),\n",
       " ('linguine', 1),\n",
       " ('cajun', 1),\n",
       " ('lt', 1),\n",
       " ('elixir', 1),\n",
       " ('pqq', 1),\n",
       " ('quaker', 1),\n",
       " ('parma', 1),\n",
       " ('bystolic', 1),\n",
       " ('sum', 1),\n",
       " ('dim sum', 1),\n",
       " ('juiced', 1),\n",
       " ('cane', 1),\n",
       " ('michelob', 1),\n",
       " ('michelob ultra', 1),\n",
       " ('healthy', 1),\n",
       " ('manchego', 1),\n",
       " ('groat', 1),\n",
       " ('muenster', 1),\n",
       " ('frosting', 1),\n",
       " ('marble', 1),\n",
       " ('kimchee', 1),\n",
       " ('blanc', 1),\n",
       " ('sauvignon blanc', 1),\n",
       " ('supp', 1),\n",
       " ('topo', 1),\n",
       " ('chico', 1),\n",
       " ('topo chico', 1),\n",
       " ('nutmeg', 1),\n",
       " ('fatty', 1),\n",
       " ('omega fatty', 1),\n",
       " ('fatty acid', 1),\n",
       " ('omega fatty acid', 1),\n",
       " ('bulgar', 1),\n",
       " ('whisps', 1),\n",
       " ('biltong', 1),\n",
       " ('zink', 1),\n",
       " ('shoulder', 1),\n",
       " ('scalloped', 1),\n",
       " ('budweiser', 1),\n",
       " ('ziti', 1),\n",
       " ('sala', 1),\n",
       " ('verapamil', 1),\n",
       " ('catfish', 1),\n",
       " ('robaxin', 1),\n",
       " ('miralax', 1),\n",
       " ('spindrift', 1),\n",
       " ('bel', 1),\n",
       " ('naturethroid', 1),\n",
       " ('whopper', 1),\n",
       " ('concentrate', 1),\n",
       " ('nan', 1),\n",
       " ('pink lady', 1),\n",
       " ('diner', 1),\n",
       " ('fake', 1),\n",
       " ('kevita', 1),\n",
       " ('strong', 1),\n",
       " ('combo', 1),\n",
       " ('cla', 1),\n",
       " ('sug', 1),\n",
       " ('macha', 1),\n",
       " ('contrave', 1),\n",
       " ('lialda', 1),\n",
       " ('delight', 1),\n",
       " ('hydroxyzine', 1),\n",
       " ('hummous', 1),\n",
       " ('van', 1),\n",
       " ('thick', 1),\n",
       " ('algae', 1),\n",
       " ('armor', 1),\n",
       " ('armor thyroid', 1),\n",
       " ('carob', 1),\n",
       " ('capuchino', 1),\n",
       " ('sleepy', 1),\n",
       " ('sleepy time', 1),\n",
       " ('gel', 1),\n",
       " ('frank', 1),\n",
       " ('sal', 1),\n",
       " ('serrapeptase', 1),\n",
       " ('spritzer', 1),\n",
       " ('moscato', 1),\n",
       " ('essential', 1),\n",
       " ('pom', 1),\n",
       " ('croquette', 1),\n",
       " ('air', 1),\n",
       " ('russian', 1),\n",
       " ('arby', 1),\n",
       " (\"arby 's\", 1),\n",
       " ('empanadas', 1),\n",
       " ('caffine', 1),\n",
       " ('sucralose', 1),\n",
       " ('button', 1),\n",
       " ('mild', 1),\n",
       " ('skittle', 1),\n",
       " ('triscuit', 1),\n",
       " ('gammon', 1),\n",
       " ('newton', 1),\n",
       " ('flower', 1),\n",
       " ('protonix', 1),\n",
       " ('vitex', 1),\n",
       " ('turnover', 1),\n",
       " ('pat', 1),\n",
       " ('tandoori', 1),\n",
       " ('yakult', 1),\n",
       " ('seedless', 1),\n",
       " ('meatless', 1),\n",
       " ('mongolian', 1),\n",
       " ('pregabalin', 1),\n",
       " ('gallon', 1),\n",
       " ('niacinamide', 1),\n",
       " ('sprinkle', 1),\n",
       " ('halfhalf', 1),\n",
       " ('ruffle', 1),\n",
       " ('tabouli', 1),\n",
       " ('bubbly', 1),\n",
       " ('cadbury', 1),\n",
       " ('angus', 1),\n",
       " ('think', 1),\n",
       " ('nakd', 1),\n",
       " ('nakd bar', 1),\n",
       " ('manhattan', 1),\n",
       " ('glimepiride', 1),\n",
       " ('earth', 1),\n",
       " ('earth balance', 1),\n",
       " ('burrata', 1),\n",
       " ('gray', 1),\n",
       " ('earl gray', 1),\n",
       " ('popper', 1),\n",
       " ('seasoned', 1),\n",
       " ('xl', 1),\n",
       " ('lactaid', 1),\n",
       " ('spare', 1),\n",
       " ('starburst', 1),\n",
       " ('york', 1),\n",
       " ('motrin', 1),\n",
       " ('velvet', 1),\n",
       " ('glutathione', 1),\n",
       " ('famotidine', 1),\n",
       " ('artificial', 1),\n",
       " ('suja', 1),\n",
       " ('brea', 1),\n",
       " ('concerta', 1),\n",
       " ('battered', 1),\n",
       " ('tiny', 1),\n",
       " ('lb', 1),\n",
       " ('smores', 1),\n",
       " ('kiwifruit', 1),\n",
       " ('moroccan', 1),\n",
       " ('rockstar', 1),\n",
       " ('polish', 1),\n",
       " ('zeaxanthin', 1),\n",
       " ('lassi', 1),\n",
       " ('balm', 1),\n",
       " ('fortune', 1),\n",
       " ('vermicelli', 1),\n",
       " ('arnold', 1),\n",
       " ('palmer', 1),\n",
       " ('arnold palmer', 1),\n",
       " ('tapenade', 1),\n",
       " ('rd', 1),\n",
       " ('mojito', 1),\n",
       " ('stevia drop', 1),\n",
       " ('flourless', 1),\n",
       " ('lily', 1),\n",
       " ('ton', 1),\n",
       " ('chuck', 1),\n",
       " ('fresco', 1),\n",
       " ('zuccini', 1),\n",
       " ('move', 1),\n",
       " ('arginine', 1),\n",
       " ('l arginine', 1),\n",
       " ('ritalin', 1),\n",
       " ('telmisartan', 1),\n",
       " ('still', 1),\n",
       " ('power bar', 1),\n",
       " ('cannabis', 1),\n",
       " ('ozarka', 1),\n",
       " ('sunkist', 1),\n",
       " ('high fat', 1),\n",
       " ('taquitos', 1),\n",
       " ('endive', 1),\n",
       " ('pasty', 1),\n",
       " ('crushed', 1),\n",
       " ('pearl', 1),\n",
       " ('kim chi', 1),\n",
       " ('jimmy john', 1),\n",
       " ('nutrisystem', 1),\n",
       " ('strength', 1),\n",
       " ('tabasco', 1),\n",
       " ('belgian', 1),\n",
       " ('su', 1),\n",
       " ('cool whip', 1),\n",
       " ('chickfila', 1),\n",
       " ('jogurt', 1),\n",
       " ('jasmin', 1),\n",
       " ('lansoprazole', 1),\n",
       " ('valerian', 1),\n",
       " ('serrano', 1),\n",
       " ('zoodles', 1),\n",
       " ('borscht', 1),\n",
       " ('vin', 1),\n",
       " ('snicker bar', 1),\n",
       " ('easter', 1),\n",
       " ('lavender', 1),\n",
       " ('brittle', 1),\n",
       " ('trim', 1),\n",
       " ('roe', 1),\n",
       " ('kung', 1),\n",
       " ('pao', 1),\n",
       " ('kung pao', 1),\n",
       " ('caffee', 1),\n",
       " ('katsu', 1),\n",
       " ('rucola', 1),\n",
       " ('yogourt', 1),\n",
       " ('quercetin', 1),\n",
       " ('marsala', 1),\n",
       " ('platter', 1),\n",
       " ('sweetners', 1),\n",
       " ('supps', 1),\n",
       " ('mtc', 1),\n",
       " ('suger', 1),\n",
       " ('swede', 1),\n",
       " ('daikon', 1),\n",
       " ('soybean', 1),\n",
       " ('liposomal', 1),\n",
       " ('uncured', 1),\n",
       " ('saag', 1),\n",
       " ('xyzal', 1),\n",
       " ('ferrous', 1),\n",
       " ('sulfate', 1),\n",
       " ('ferrous sulfate', 1),\n",
       " ('liverwurst', 1),\n",
       " ('chlorophyll', 1),\n",
       " ('manuka', 1),\n",
       " ('pulp', 1),\n",
       " ('cre', 1),\n",
       " ('rosehip', 1),\n",
       " ('radicchio', 1),\n",
       " ('epic bar', 1),\n",
       " ('margherita', 1),\n",
       " ('praline', 1),\n",
       " ('canteloupe', 1),\n",
       " ('savoury', 1),\n",
       " ('fasting', 1),\n",
       " ('fishoil', 1),\n",
       " ('thee', 1),\n",
       " ('true', 1),\n",
       " ('cyclobenzaprine', 1),\n",
       " ('perogies', 1),\n",
       " ('granary', 1),\n",
       " ('chana', 1),\n",
       " ('diltiazem', 1),\n",
       " ('romain', 1),\n",
       " ('lf', 1),\n",
       " ('snapper', 1),\n",
       " ('antioxidant', 1),\n",
       " ('five', 1),\n",
       " ('malbec', 1),\n",
       " ('nuttzo', 1),\n",
       " ('release', 1),\n",
       " ('time release', 1),\n",
       " ('restaurant', 1),\n",
       " ('stilton', 1),\n",
       " ('inositol', 1),\n",
       " ('goddess', 1),\n",
       " ('tomate', 1),\n",
       " ('rom', 1),\n",
       " ('raisens', 1),\n",
       " ('advocado', 1),\n",
       " ('night time', 1),\n",
       " ('halloween', 1),\n",
       " ('chix', 1),\n",
       " ('riboflavin', 1),\n",
       " ('benefiber', 1),\n",
       " ('sr', 1),\n",
       " ('roibos', 1),\n",
       " ('giant', 1),\n",
       " ('frappe', 1),\n",
       " ('branched', 1),\n",
       " ('chain', 1),\n",
       " ('branched chain', 1),\n",
       " ('pak', 1),\n",
       " ('pak choi', 1),\n",
       " ('edam', 1),\n",
       " ('gut', 1),\n",
       " ('daves', 1),\n",
       " ('killer', 1),\n",
       " ('daves killer', 1),\n",
       " ('shreddies', 1),\n",
       " ('crackling', 1),\n",
       " ('broiled', 1),\n",
       " ('jerk', 1),\n",
       " ('field', 1),\n",
       " ('ltheanine', 1),\n",
       " ('nortriptyline', 1),\n",
       " ('flavoring', 1),\n",
       " ('wakame', 1),\n",
       " ('fexofenadine', 1),\n",
       " ('antacid', 1),\n",
       " ('gotu', 1),\n",
       " ('kola', 1),\n",
       " ('gotu kola', 1),\n",
       " ('hyaluronic', 1),\n",
       " ('hyaluronic acid', 1),\n",
       " ('lungo', 1),\n",
       " ('cohosh', 1),\n",
       " ('taurate', 1),\n",
       " ('monkfruit', 1),\n",
       " ('lo', 1),\n",
       " ('lo mein', 1),\n",
       " ('ripple', 1),\n",
       " ('kaffee', 1),\n",
       " ('biosil', 1),\n",
       " ('advocare', 1),\n",
       " ('advocare spark', 1),\n",
       " ('lcarnitine', 1),\n",
       " ('taro', 1),\n",
       " ('benedryl', 1),\n",
       " ('wake', 1),\n",
       " ('kaki', 1),\n",
       " ('chimichanga', 1),\n",
       " ('freeze', 1),\n",
       " ('cognac', 1),\n",
       " ('sunbutter', 1),\n",
       " ('lasix', 1),\n",
       " ('tuscan', 1),\n",
       " ('upma', 1),\n",
       " ('b vit', 1),\n",
       " ('jalepeno', 1),\n",
       " ('country', 1),\n",
       " ('porrige', 1),\n",
       " ('halfnhalf', 1),\n",
       " ('garcinia', 1),\n",
       " ('cambogia', 1),\n",
       " ('garcinia cambogia', 1),\n",
       " ('ha', 1),\n",
       " ('limeade', 1),\n",
       " ('filling', 1),\n",
       " ('cocacola', 1),\n",
       " ('nystatin', 1),\n",
       " ('meusli', 1),\n",
       " ('horchata', 1),\n",
       " ('victoza', 1),\n",
       " ('billion', 1),\n",
       " ('bleu', 1),\n",
       " ('isopure', 1),\n",
       " ('alanine', 1),\n",
       " ('beta alanine', 1),\n",
       " ('balsamico', 1),\n",
       " ('cuke', 1),\n",
       " ('mozzerella', 1),\n",
       " ('start', 1),\n",
       " ('th', 1),\n",
       " ('vita', 1),\n",
       " ('nothing', 1),\n",
       " ('liquor', 1),\n",
       " ('sotalol', 1),\n",
       " ('hvy', 1),\n",
       " ('forest', 1),\n",
       " ('keifer', 1),\n",
       " ('generic', 1),\n",
       " ('boron', 1),\n",
       " ('peel', 1),\n",
       " ('statin', 1),\n",
       " ('werthers', 1),\n",
       " ('cr', 1),\n",
       " ('lucky', 1),\n",
       " ('charm', 1),\n",
       " ('lucky charm', 1),\n",
       " ('orotate', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_new_phrases_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter-off \"sub-grams\"\n",
    "### Construct sub-gram bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sub_grams = []\n",
    "for ph in known_phrase_dict.keys():\n",
    "    tokens = nltk.word_tokenize(ph)\n",
    "    for tok in tokens:\n",
    "        all_sub_grams.append((tok, known_phrase_dict[ph]))\n",
    "    if len(tokens) == 1:\n",
    "        continue\n",
    "        \n",
    "    for n in [2, 3, 4, 5]:\n",
    "        for sub_gram in nltk.ngrams(tokens, n):\n",
    "            all_sub_grams.append((' '.join(sub_gram), known_phrase_dict[ph]))\n",
    "\n",
    "sub_gram_freq = defaultdict(lambda : 0)\n",
    "# Accumulate frequency\n",
    "for pair in all_sub_grams:\n",
    "    sub_gram_freq[pair[0]] += pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_gram_freq['apple cider']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter off them from new frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_new_phrases = []\n",
    "for pair in sorted_new_phrases:\n",
    "    if pair[0] in sub_gram_freq.keys():\n",
    "        filtered_new_phrases.append((pair[0], pair[1] - sub_gram_freq[pair[0]]))\n",
    "    else:\n",
    "        filtered_new_phrases.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loratadine', 247),\n",
       " ('crestor', 243),\n",
       " ('evening med', 239),\n",
       " ('allegra', 236),\n",
       " ('inhaler', 235),\n",
       " ('cymbalta', 232),\n",
       " ('pressure med', 231),\n",
       " ('prednisone', 230),\n",
       " ('folate', 230),\n",
       " ('prozac', 226)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_filtered_new_phrases = sorted(filtered_new_phrases, key = lambda x: x[1], reverse = True)\n",
    "sorted_filtered_new_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159990"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_filtered_new_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in sorted_filtered_new_phrases if i[1] > 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in sorted_filtered_new_phrases if i[1] > 750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in sorted_filtered_new_phrases if i[1] > 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loratadine', 247), ('crestor', 243), ('evening med', 239)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_filtered_new_phrases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labeled_phrases = pd.read_csv('../foodparser/data/combined_gram_set copy.csv').gram_key.values\n",
    "# unlabeled_phrases = [i for i in sorted_filtered_new_phrases if i[0] not in labeled_phrases and i[1] > 250]\n",
    "# len(unlabeled_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', 12170), ('b', 7861), ('g', 7277)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unlabeled_phrases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabeled_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = [pair[0] for pair in unlabeled_phrases]\n",
    "# gram_type = [len(nltk.word_tokenize(pair[0])) for pair in unlabeled_phrases]\n",
    "# count = [pair[1] for pair in unlabeled_phrases]\n",
    "\n",
    "# additional_phrases = pd.DataFrame({\n",
    "#     'key': key,\n",
    "#     'gram_type': gram_type,\n",
    "#     'count': count\n",
    "# }).sort_values(by = 'gram_type', ascending = False)\n",
    "\n",
    "# additional_phrases.to_csv(result_dir + 'unlabeled_phrases.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram_key</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ml</td>\n",
       "      <td>10365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>herbal</td>\n",
       "      <td>7482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>breakfast</td>\n",
       "      <td>5925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>white</td>\n",
       "      <td>5684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dinner</td>\n",
       "      <td>5460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     gram_key  counts\n",
       "0          ml   10365\n",
       "1      herbal    7482\n",
       "2   breakfast    5925\n",
       "3       white    5684\n",
       "4      dinner    5460"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncatched_tokens = pd.read_csv(result_dir + 'all_unknown_phrases.csv').query('counts > 250')\n",
    "# uncatched_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_known_phrases = pd.read_csv('../foodparser/data/combined_gram_set.csv').gram_key.values\n",
    "# additional_unlabeled_phrases = [i.strip() for i in uncatched_tokens.gram_key.values]\n",
    "# additional_unlabeled_phrases = [i for i in additional_unlabeled_phrases if i not in current_known_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(additional_unlabeled_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(additional_unlabeled_phrases, columns = ['gram_key'])\\\n",
    "#     .to_csv(result_dir + 'additional_unlabeled_phrases.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>gram_type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fish oil multi vitamin</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vitamin b complex vitamin</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vitamin c fish oil</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fish oil vitamin b</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multi vitamin fish oil</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         key  gram_type  count\n",
       "0     fish oil multi vitamin          4     84\n",
       "1  vitamin b complex vitamin          4     89\n",
       "2         vitamin c fish oil          4     60\n",
       "3         fish oil vitamin b          4     65\n",
       "4     multi vitamin fish oil          4     60"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_phrases = [i for i in sorted_filtered_new_phrases if i[1] > 50]\n",
    "\n",
    "key = [pair[0] for pair in selected_phrases]\n",
    "gram_type = [len(nltk.word_tokenize(pair[0])) for pair in selected_phrases]\n",
    "count = [pair[1] for pair in selected_phrases]\n",
    "\n",
    "additional_phrases = pd.DataFrame({\n",
    "    'key': key,\n",
    "    'gram_type': gram_type,\n",
    "    'count': count\n",
    "}).sort_values(by = 'gram_type', ascending = False).reset_index(drop = True)\n",
    "additional_phrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, (826, 3))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_phrases['count'].max(), additional_phrases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = pd.read_csv('../foodparser/data/combined_gram_set.csv')\n",
    "# combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.query('food_type != \"n\"').reset_index(drop = True).to_csv('../foodparser/data/combined_gram_set.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.query('food_type != \"n\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_phrases.to_csv(result_dir + 'additional_phrases.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency cutoff to maximize entry rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_new_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'egg plant' in [pair[0] for pair in sorted_new_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60951"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignoring all the phrases with less than 10 apperances\n",
    "all_phrase_freq = [pair for pair in sorted_old_phrases + sorted_new_phrases if pair[1] > 10]\n",
    "len(all_phrase_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700), ('coffee', 142167), ('tea', 83643)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrase_freq[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700, 1), ('coffee', 142167, 1), ('tea', 83643, 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrase_freq_w_length = [(pair[0], pair[1], len(pair[0].split())) for pair in all_phrase_freq]\n",
    "all_phrase_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8286"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unigram_freq_w_length = [pair for pair in all_phrase_freq_w_length if pair[2] == 1]\n",
    "len(all_unigram_freq_w_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700, 1), ('coffee', 142167, 1), ('tea', 83643, 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unigram_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cold brew coffee almond milk', 11, 5),\n",
       " ('olive oil apple cider vinegar', 65, 5),\n",
       " ('whole wheat bread peanut butter', 58, 5)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_all_phrase_freq_w_length = sorted(all_phrase_freq_w_length, key = lambda x: x[2], reverse = True)\n",
    "sorted_all_phrase_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653926"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entry_rate(gram_set, training_texts):\n",
    "    tmp_lst = [pair[0] for pair in training_texts]\n",
    "    prev_length = len(tmp_lst)\n",
    "    \n",
    "    total_entry_matched = 0\n",
    "    for gram in tqdm(gram_set):\n",
    "        tmp_lst =  [entry for entry in tmp_lst if gram not in entry.split()]\n",
    "        curr_length = len(tmp_lst)\n",
    "        total_entry_matched += (prev_length - curr_length)\n",
    "        prev_length = curr_length\n",
    "    return total_entry_matched / len(training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9183627320690285"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_grams = [pair[0] for pair in all_unigram_freq_w_length[:500]]\n",
    "calc_entry_rate(tmp_grams, training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700, 1), ('coffee', 142167, 1), ('tea', 83643, 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unigram_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed_token_bank = set([pair[0] for pair in all_unigram_freq_w_length[:500]])\n",
    "# len(list(needed_token_bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed_phrases = []\n",
    "# unique_tokens = defaultdict(lambda : 0)\n",
    "# for pair in sorted_all_phrase_freq_w_length:\n",
    "#     found = False\n",
    "#     for token in pair[0].split():\n",
    "#         if token in needed_token_bank:\n",
    "#             unique_tokens[token] += 1\n",
    "#             found = True\n",
    "#     if found and pair[0] not in known_grams_set:\n",
    "#         needed_phrases.append(pair)\n",
    "#     if len(dict(unique_tokens).keys()) >= 500:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(needed_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_knowledge_base = list(prepare_phrase_base(known_grams_set))\n",
    "# all_token = []\n",
    "# for i in tmp_knowledge_base:\n",
    "#     for token in i.split():\n",
    "#         if token not in all_token:\n",
    "#             all_token.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_needed_phrases = [pair[0] for pair in needed_phrases]\n",
    "# mask = [True] * len(all_needed_phrases)\n",
    "# for idx in tqdm(range(len(all_needed_phrases) - 1, -1, -1)):\n",
    "#     curr_ph = all_needed_phrases[idx]\n",
    "#     for ph in all_needed_phrases:\n",
    "#         if ph != curr_ph:\n",
    "#             if curr_ph + ' ' in ph or ' ' + curr_ph in ph:\n",
    "#                 mask[idx] = False\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.array(all_needed_phrases)[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_known_phrases_lst = []\n",
    "# for file in ['quad_grams.csv', 'bi_grams.csv', 'uni_grams.csv', 'tri_grams.csv', 'penta_grams.csv']:\n",
    "#     curr_df = pd.read_csv('../resources/' + file)\n",
    "#     curr_phrases = curr_df.key.values\n",
    "#     all_known_phrases_lst += list(curr_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_known_phrases_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = [pair[0] for pair in sorted_items]\n",
    "gram_type = [len(nltk.word_tokenize(pair[0])) for pair in sorted_items]\n",
    "count = [pair[1] for pair in sorted_items]\n",
    "\n",
    "additional_phrases = pd.DataFrame({\n",
    "    'key': key,\n",
    "    'gram_type': gram_type,\n",
    "    'count': count\n",
    "}).sort_values(by = 'gram_type', ascending = False)\n",
    "\n",
    "additional_phrases.to_csv('../data/result/additional_phrases.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ml', 'herbal', 'b'], [1, 1, 1], [12170, 8492, 7861])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key[:3], gram_type[:3], count[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ml', 'herbal', 'b'], 1731)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_phrase_bank = [pair[0] for pair in sorted_items]\n",
    "curr_phrase_bank[:3], len(curr_phrase_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1731 [00:00<01:04, 26.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Length of curr_phrase_bank: 1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1731/1731 [03:24<00:00,  8.47it/s]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-52-2270ebb5ba5d>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-2270ebb5ba5d>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "print('=> Length of curr_phrase_bank:', len(curr_phrase_bank))\n",
    "    \n",
    "tmp_mask = np.array([True] * len(curr_phrase_bank))\n",
    "curr_phrase_bank = np.array(curr_phrase_bank)\n",
    "    \n",
    "curr_idx = 0\n",
    "for ph in tqdm(curr_phrase_bank):\n",
    "    tokens = nltk.word_tokenize(ph)\n",
    "    curr_gram_length = len(tokens)\n",
    "    for other_ph in curr_phrase_bank:\n",
    "        if ph == other_ph:\n",
    "            continue\n",
    "            \n",
    "        other_ph_tokens = nltk.word_tokenize(other_ph)\n",
    "        if len(other_ph_tokens) < curr_gram_length:\n",
    "            continue\n",
    "            \n",
    "        n_grams = [' '.join(gram) for gram in nltk.ngrams(other_ph_tokens, curr_gram_length)]\n",
    "        if ph in n_grams:\n",
    "            tmp_mask[curr_idx] = False\n",
    "            break\n",
    "    curr_idx += 1\n",
    "if sum(tmp_mask) == len(tmp_mask):\n",
    "    print('    => No Change to current phrase bank!')\n",
    "#     break\n",
    "curr_phrase_bank = curr_phrase_bank[tmp_mask]\n",
    "print(len(curr_phrase_bank))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413\n"
     ]
    }
   ],
   "source": [
    "curr_phrase_bank = curr_phrase_bank[tmp_mask]\n",
    "\n",
    "print(len(curr_phrase_bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
