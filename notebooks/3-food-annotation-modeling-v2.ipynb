{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict \n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '../'\n",
    "\n",
    "data_dir = file_dir + 'test_data/'\n",
    "raw_data_dir = data_dir + 'raw/'\n",
    "result_dir = data_dir + 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(1, '../foodparser/')\n",
    "\n",
    "from foodparser import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = FoodParser()\n",
    "# fp.initialization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp.run_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/joey-\n",
      "[nltk_data]     hou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joey-hou/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords, words\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# Updated by Katherine August 23rd 2020\n",
    "stop_words.remove('out') # since pre work out is a valid beverage name\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "\n",
    "wordlist = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting / Making test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>original_logtime_notz</th>\n",
       "      <th>day</th>\n",
       "      <th>local_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>282</td>\n",
       "      <td>alqt15090005</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>09-09-2015 8:20PM JST</td>\n",
       "      <td>2015-09-09 20:20:00</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>20:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>283</td>\n",
       "      <td>alqt15090005</td>\n",
       "      <td>150</td>\n",
       "      <td>Soup</td>\n",
       "      <td>f</td>\n",
       "      <td>09-09-2015 10:38PM JST</td>\n",
       "      <td>2015-09-09 22:38:00</td>\n",
       "      <td>2015-09-09</td>\n",
       "      <td>22.633333</td>\n",
       "      <td>22:38:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID   unique_code  research_info_id desc_text food_type  \\\n",
       "281  282  alqt15090005               150     Water         w   \n",
       "282  283  alqt15090005               150      Soup         f   \n",
       "\n",
       "           original_logtime original_logtime_notz         day  local_time  \\\n",
       "281   09-09-2015 8:20PM JST   2015-09-09 20:20:00  2015-09-09   20.333333   \n",
       "282  09-09-2015 10:38PM JST   2015-09-09 22:38:00  2015-09-09   22.633333   \n",
       "\n",
       "         time  week_from_start  year  \n",
       "281  20:20:00                1  2015  \n",
       "282  22:38:00                1  2015  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_all_pickle_file = open(raw_data_dir + 'public_0924_basline_usable_expanded.pickle', 'rb')\n",
    "public_all = pickle.load(public_all_pickle_file)\n",
    "public_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = public_all.sample(2000)[['desc_text', 'food_type']].copy().reset_index(drop = True)\n",
    "# test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.to_csv(raw_data_dir + 'test_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_all.reset_index(drop = True).to_csv(raw_data_dir + 'all_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1653926, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loggings = public_all.desc_text.values\n",
    "all_entries = []\n",
    "for l in all_loggings:\n",
    "    if ',' in l:\n",
    "        for entry in l.split(','):\n",
    "            all_entries.append(entry.strip())\n",
    "    else:\n",
    "        all_entries.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2399626"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entries_length = ([len(i.split()) for i in all_entries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(all_entries_length) > 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional training texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764077, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unknown_phrases_df = pd.read_csv(result_dir + 'all_unknown_phrases.csv').dropna()\n",
    "all_unknown_phrases_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unknown_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>isotonic drink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sashimi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unknown_phrases\n",
       "0  isotonic drink\n",
       "1         sashimi"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unknown_phrases_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building improved food annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FoodParser()\n",
    "fp.initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load known grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram_key</th>\n",
       "      <th>food_type</th>\n",
       "      <th>gram_type</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>tag3</th>\n",
       "      <th>tag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chilli</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chile</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gram_key food_type  gram_type tag1 tag2 tag3 tag4\n",
       "0   chilli         f          1  NaN  NaN  NaN  NaN\n",
       "1    chile         f          1  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_grams = pd.read_csv('../foodparser/data/combined_gram_set.csv')\n",
    "known_grams.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gram_key</th>\n",
       "      <th>food_type</th>\n",
       "      <th>gram_type</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>tag3</th>\n",
       "      <th>tag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>curcumin</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>apple cider</td>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gram_key food_type  gram_type tag1 tag2 tag3 tag4\n",
       "1100     curcumin         m          1  NaN  NaN  NaN  NaN\n",
       "1101  apple cider         b          2  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_grams.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_grams_lst = list(known_grams.gram_key.values)\n",
    "known_grams_set = set(known_grams_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['oc', 'amp', 'homemade', 'food', 'foodporn', 'v', 'x', 'recipe', 'best', 'il', 'ever',\n",
    "                'time', 'attempt', 'first', 'ate', 'made', 'home', 'today', 'make', 'friend', 'local',\n",
    "                'new', 'day', 'birthday', 'like', 'amazing', 'de', 'happy', 'year', 'plate', 'video', \n",
    "                'cooked', 'dish', 'house', 'os', 'tried', 'super', 'perfect', 'way', 'delicious', \n",
    "                'night', 'last', 'eating', 'know', \n",
    "                 'mg', 'oz', 'and', 'w', 'x', 'n', 'ml'\n",
    "                 'handful']\n",
    "\n",
    "# Function for removing numbers\n",
    "def handle_numbers(text):\n",
    "    clean_text = text\n",
    "    clean_text = re.sub('[0-9]+\\.[0-9]+', '' , clean_text)\n",
    "    clean_text = re.sub('[0-9]+', '' , clean_text)\n",
    "    clean_text = re.sub('\\s\\s+', ' ', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "# Function for removing punctuation\n",
    "def drop_punc(text):\n",
    "    clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    return clean_text\n",
    "\n",
    "# Remove normal stopwords\n",
    "def remove_stop(my_text):\n",
    "    text_list = my_text.split()\n",
    "    return ' '.join([word for word in text_list if word not in stop_words])\n",
    "\n",
    "def remove_my_stop(text):\n",
    "    text_list = text.split()\n",
    "    return ' '.join([word for word in text_list if word not in my_stop_words])\n",
    "\n",
    "def remove_plural(w):\n",
    "    return wnl.lemmatize(w)\n",
    "\n",
    "def pre_processing(text):\n",
    "    text = text.lower()\n",
    "    text = remove_my_stop(remove_stop(handle_numbers(drop_punc(text)))).strip()\n",
    "    return ' '.join([remove_plural(w) for w in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = public_all.desc_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653926/1653926 [00:52<00:00, 31748.94it/s]\n"
     ]
    }
   ],
   "source": [
    "all_text_cleaned = [pre_processing(tx) for tx in tqdm(all_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653926"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water', 'soup', 'yogurt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_cleaned[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'f', 'b']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_food_type = list(public_all.food_type.values)\n",
    "all_food_type[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 'w'), ('soup', 'f'), ('yogurt', 'b')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_texts = list(zip(all_text_cleaned, all_food_type))\n",
    "training_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement food mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chia squeeze', 'greek yogurt granola', 'vitamin k']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(known_grams_set)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_single_phrase(ph, curr_phrase_dict, known_phrase_dict, known_sub_phrases_set):\n",
    "    # Check known phrases list\n",
    "    try:\n",
    "        known_phrase_dict[ph] += 1\n",
    "        return 0\n",
    "    except:\n",
    "        pass\n",
    "    # Add to current phrase list if its not a subphrase\n",
    "    if ph not in known_sub_phrases_set:\n",
    "        curr_phrase_dict[ph] += 1\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prepare_phrase_base(known_grams_set):\n",
    "    all_known_phrases_lst = []\n",
    "    for file in ['quad_grams.csv', 'bi_grams.csv', 'uni_grams.csv', 'tri_grams.csv', 'penta_grams.csv']:\n",
    "        curr_df = pd.read_csv('../foodparser/data/annotations/' + file)\n",
    "        curr_phrases = curr_df.key.values\n",
    "        all_known_phrases_lst += list(curr_phrases)\n",
    "    all_known_sub_phrases_set = set(all_known_phrases_lst)\n",
    "    return all_known_sub_phrases_set\n",
    "\n",
    "def run_training(known_grams_set, training_texts):\n",
    "    '''\n",
    "    Input format:\n",
    "        - known_grams_set: set object, e.g. {'water', 'preworkout', ...}\n",
    "        - training_texts: list object, e.g. [('water', 'w'), ('preworkout', 'b'), ...]\n",
    "    '''\n",
    "    \n",
    "    # Make a spellChecker instance\n",
    "    # spell = SpellChecker()\n",
    "\n",
    "    # Known phrases \n",
    "    known_phrase_dict = dict(zip(known_grams_set, [0] * len(list(known_grams_set))))\n",
    "    known_sub_phrases_set = prepare_phrase_base(known_grams_set)\n",
    "    \n",
    "    # Scanning through the documents\n",
    "    curr_phrase_dict = defaultdict(lambda : 0)\n",
    "\n",
    "    for doc_pair in tqdm(training_texts):\n",
    "        if len(doc_pair) == 2:\n",
    "            doc, food_type = doc_pair\n",
    "        else:\n",
    "            doc = doc_pair\n",
    "        tokens = nltk.word_tokenize(doc)\n",
    "        \n",
    "        # Uni-grams\n",
    "        [handle_single_phrase(ph, curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in tokens]\n",
    "        \n",
    "        # N-grams\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 2)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 3)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 4)]\n",
    "        [handle_single_phrase(' '.join(list(ph)), curr_phrase_dict, known_phrase_dict, known_sub_phrases_set) for ph in nltk.ngrams(tokens, 5)]\n",
    "        \n",
    "    return dict(known_phrase_dict), dict(curr_phrase_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653926/1653926 [02:43<00:00, 10093.16it/s]\n",
      "100%|██████████| 764077/764077 [01:08<00:00, 11220.47it/s]\n"
     ]
    }
   ],
   "source": [
    "known_phrase_dict, curr_phrase_dict = run_training(known_grams_set, training_texts)\n",
    "add_known_phrase_dict, add_curr_phrase_dict = run_training(known_grams_set, all_unknown_phrases_df.unknown_phrases.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1102, 2181428)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_phrase_dict = dict(curr_phrase_dict)\n",
    "known_phrase_dict = dict(known_phrase_dict)\n",
    "\n",
    "sorted_old_phrases = sorted(known_phrase_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_new_phrases = sorted(curr_phrase_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "\n",
    "len(sorted_old_phrases), len(sorted_new_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_curr_phrase_dict = dict(add_curr_phrase_dict)\n",
    "add_known_phrase_dict = dict(add_known_phrase_dict)\n",
    "\n",
    "sorted_old_phrases_add = sorted(add_known_phrase_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_new_phrases_add = sorted(add_curr_phrase_dict.items(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_new_phrases_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter-off \"sub-grams\"\n",
    "### Construct sub-gram bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sub_grams = []\n",
    "for ph in known_phrase_dict.keys():\n",
    "    tokens = nltk.word_tokenize(ph)\n",
    "    for tok in tokens:\n",
    "        all_sub_grams.append((tok, known_phrase_dict[ph]))\n",
    "    if len(tokens) == 1:\n",
    "        continue\n",
    "        \n",
    "    for n in [2, 3, 4, 5]:\n",
    "        for sub_gram in nltk.ngrams(tokens, n):\n",
    "            all_sub_grams.append((' '.join(sub_gram), known_phrase_dict[ph]))\n",
    "\n",
    "sub_gram_freq = defaultdict(lambda : 0)\n",
    "# Accumulate frequency\n",
    "for pair in all_sub_grams:\n",
    "    sub_gram_freq[pair[0]] += pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6030"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_gram_freq['apple cider']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter off them from new frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_new_phrases = []\n",
    "for pair in sorted_new_phrases:\n",
    "    if pair[0] in sub_gram_freq.keys():\n",
    "        filtered_new_phrases.append((pair[0], pair[1] - sub_gram_freq[pair[0]]))\n",
    "    else:\n",
    "        filtered_new_phrases.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ml', 7508),\n",
       " ('g', 7277),\n",
       " ('free', 7261),\n",
       " ('no', 5634),\n",
       " ('b', 4851),\n",
       " ('med', 4819),\n",
       " ('crisp', 3353),\n",
       " ('stevia', 3255),\n",
       " ('unsweetened', 3079),\n",
       " ('kefir', 2986)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_filtered_new_phrases = sorted(filtered_new_phrases, key = lambda x: x[1], reverse = True)\n",
    "sorted_filtered_new_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in sorted_filtered_new_phrases if i[1] > 250 and i[1] < 750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_phrases = [i for i in sorted_filtered_new_phrases if i[1] > 250]\n",
    "\n",
    "key = [pair[0] for pair in selected_phrases]\n",
    "gram_type = [len(nltk.word_tokenize(pair[0])) for pair in selected_phrases]\n",
    "count = [pair[1] for pair in selected_phrases]\n",
    "\n",
    "additional_phrases = pd.DataFrame({\n",
    "    'key': key,\n",
    "    'gram_type': gram_type,\n",
    "    'count': count\n",
    "}).sort_values(by = 'gram_type', ascending = False)\n",
    "\n",
    "additional_phrases.to_csv(result_dir + 'additional_phrases.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = pd.read_csv('../foodparser/data/combined_gram_set.csv')\n",
    "# combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.query('food_type != \"n\"').reset_index(drop = True).to_csv('../foodparser/data/combined_gram_set.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.query('food_type != \"n\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict(sorted_filtered_new_phrases)['apple']\n",
    "50 * 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency cutoff to maximize entry rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_new_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'egg plant' in [pair[0] for pair in sorted_new_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60951"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignoring all the phrases with less than 10 apperances\n",
    "all_phrase_freq = [pair for pair in sorted_old_phrases + sorted_new_phrases if pair[1] > 10]\n",
    "len(all_phrase_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700), ('coffee', 142167), ('tea', 83643)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrase_freq[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700, 1), ('coffee', 142167, 1), ('tea', 83643, 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_phrase_freq_w_length = [(pair[0], pair[1], len(pair[0].split())) for pair in all_phrase_freq]\n",
    "all_phrase_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8286"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unigram_freq_w_length = [pair for pair in all_phrase_freq_w_length if pair[2] == 1]\n",
    "len(all_unigram_freq_w_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700, 1), ('coffee', 142167, 1), ('tea', 83643, 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unigram_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cold brew coffee almond milk', 11, 5),\n",
       " ('olive oil apple cider vinegar', 65, 5),\n",
       " ('whole wheat bread peanut butter', 58, 5)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_all_phrase_freq_w_length = sorted(all_phrase_freq_w_length, key = lambda x: x[2], reverse = True)\n",
    "sorted_all_phrase_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653926"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entry_rate(gram_set, training_texts):\n",
    "    tmp_lst = [pair[0] for pair in training_texts]\n",
    "    prev_length = len(tmp_lst)\n",
    "    \n",
    "    total_entry_matched = 0\n",
    "    for gram in tqdm(gram_set):\n",
    "        tmp_lst =  [entry for entry in tmp_lst if gram not in entry.split()]\n",
    "        curr_length = len(tmp_lst)\n",
    "        total_entry_matched += (prev_length - curr_length)\n",
    "        prev_length = curr_length\n",
    "    return total_entry_matched / len(training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:24<00:00, 20.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9183627320690285"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_grams = [pair[0] for pair in all_unigram_freq_w_length[:500]]\n",
    "calc_entry_rate(tmp_grams, training_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 397700, 1), ('coffee', 142167, 1), ('tea', 83643, 1)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unigram_freq_w_length[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed_token_bank = set([pair[0] for pair in all_unigram_freq_w_length[:500]])\n",
    "# len(list(needed_token_bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed_phrases = []\n",
    "# unique_tokens = defaultdict(lambda : 0)\n",
    "# for pair in sorted_all_phrase_freq_w_length:\n",
    "#     found = False\n",
    "#     for token in pair[0].split():\n",
    "#         if token in needed_token_bank:\n",
    "#             unique_tokens[token] += 1\n",
    "#             found = True\n",
    "#     if found and pair[0] not in known_grams_set:\n",
    "#         needed_phrases.append(pair)\n",
    "#     if len(dict(unique_tokens).keys()) >= 500:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(needed_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_knowledge_base = list(prepare_phrase_base(known_grams_set))\n",
    "# all_token = []\n",
    "# for i in tmp_knowledge_base:\n",
    "#     for token in i.split():\n",
    "#         if token not in all_token:\n",
    "#             all_token.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_needed_phrases = [pair[0] for pair in needed_phrases]\n",
    "# mask = [True] * len(all_needed_phrases)\n",
    "# for idx in tqdm(range(len(all_needed_phrases) - 1, -1, -1)):\n",
    "#     curr_ph = all_needed_phrases[idx]\n",
    "#     for ph in all_needed_phrases:\n",
    "#         if ph != curr_ph:\n",
    "#             if curr_ph + ' ' in ph or ' ' + curr_ph in ph:\n",
    "#                 mask[idx] = False\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(np.array(all_needed_phrases)[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_known_phrases_lst = []\n",
    "# for file in ['quad_grams.csv', 'bi_grams.csv', 'uni_grams.csv', 'tri_grams.csv', 'penta_grams.csv']:\n",
    "#     curr_df = pd.read_csv('../resources/' + file)\n",
    "#     curr_phrases = curr_df.key.values\n",
    "#     all_known_phrases_lst += list(curr_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_known_phrases_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = [pair[0] for pair in sorted_items]\n",
    "gram_type = [len(nltk.word_tokenize(pair[0])) for pair in sorted_items]\n",
    "count = [pair[1] for pair in sorted_items]\n",
    "\n",
    "additional_phrases = pd.DataFrame({\n",
    "    'key': key,\n",
    "    'gram_type': gram_type,\n",
    "    'count': count\n",
    "}).sort_values(by = 'gram_type', ascending = False)\n",
    "\n",
    "additional_phrases.to_csv('../data/result/additional_phrases.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ml', 'herbal', 'b'], [1, 1, 1], [12170, 8492, 7861])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key[:3], gram_type[:3], count[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ml', 'herbal', 'b'], 1731)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_phrase_bank = [pair[0] for pair in sorted_items]\n",
    "curr_phrase_bank[:3], len(curr_phrase_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1731 [00:00<01:04, 26.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Length of curr_phrase_bank: 1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1731/1731 [03:24<00:00,  8.47it/s]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-52-2270ebb5ba5d>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-2270ebb5ba5d>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "print('=> Length of curr_phrase_bank:', len(curr_phrase_bank))\n",
    "    \n",
    "tmp_mask = np.array([True] * len(curr_phrase_bank))\n",
    "curr_phrase_bank = np.array(curr_phrase_bank)\n",
    "    \n",
    "curr_idx = 0\n",
    "for ph in tqdm(curr_phrase_bank):\n",
    "    tokens = nltk.word_tokenize(ph)\n",
    "    curr_gram_length = len(tokens)\n",
    "    for other_ph in curr_phrase_bank:\n",
    "        if ph == other_ph:\n",
    "            continue\n",
    "            \n",
    "        other_ph_tokens = nltk.word_tokenize(other_ph)\n",
    "        if len(other_ph_tokens) < curr_gram_length:\n",
    "            continue\n",
    "            \n",
    "        n_grams = [' '.join(gram) for gram in nltk.ngrams(other_ph_tokens, curr_gram_length)]\n",
    "        if ph in n_grams:\n",
    "            tmp_mask[curr_idx] = False\n",
    "            break\n",
    "    curr_idx += 1\n",
    "if sum(tmp_mask) == len(tmp_mask):\n",
    "    print('    => No Change to current phrase bank!')\n",
    "#     break\n",
    "curr_phrase_bank = curr_phrase_bank[tmp_mask]\n",
    "print(len(curr_phrase_bank))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413\n"
     ]
    }
   ],
   "source": [
    "curr_phrase_bank = curr_phrase_bank[tmp_mask]\n",
    "\n",
    "print(len(curr_phrase_bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
